<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Web Scraping on Jay M. Patel</title>
    <link>http://jaympatel.com/tags/web-scraping/</link>
    <description>Recent content in Web Scraping on Jay M. Patel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <copyright>Jay M. Patel</copyright>
    <lastBuildDate>Thu, 14 Feb 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://jaympatel.com/tags/web-scraping/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Introduction to web scraping in python using Beautiful Soup</title>
      <link>http://jaympatel.com/2019/02/introduction-to-web-scraping-in-python-using-beautiful-soup/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://jaympatel.com/2019/02/introduction-to-web-scraping-in-python-using-beautiful-soup/</guid>
      <description>The first step for any web scraping project is getting the webpage you want to parse. There are many python libraries such as urllib, urllib2, urllib3 for requesting pages via HTTP, however, none of them beat the elegance of requests library which we have been using in earlier posts on rest APIs and we will continue to use that here. Before we get into the workings of Beautiful Soup, let us first get a basic understanding of HTML structure, common tags and styling sheets.</description>
    </item>
    
    <item>
      <title>Why is web scraping essential and who uses web scraping?</title>
      <link>http://jaympatel.com/2019/02/why-is-web-scraping-essential-and-who-uses-web-scraping/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://jaympatel.com/2019/02/why-is-web-scraping-essential-and-who-uses-web-scraping/</guid>
      <description>Web scraping, also called web harvesting, web data extraction, or even web data mining is defined as a software program or code designed to automate the downloading and parsing of the data from the web.
Nowadays many websites such as Twitter, Facebook etc. provides REST based Application Programming Interface (APIs) to programmatically consume the structured data available on their websites and data obtained that way is usually not only &amp;ldquo;cleaner&amp;rdquo; but also easy and hassle-free compared to web scraping.</description>
    </item>
    
    <item>
      <title>Introduction data enrichment based on email addresses and domain names</title>
      <link>http://jaympatel.com/2020/08/introduction-data-enrichment-based-on-email-addresses-and-domain-names/</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>http://jaympatel.com/2020/08/introduction-data-enrichment-based-on-email-addresses-and-domain-names/</guid>
      <description>Data enrichment is a general term to denote any processes which enhance and enrich your existing database with additional information which can be used to drive your business goals such as marketing, sales and lead generation, customer relations by targeted outreach and preventing customer churn.
Plainly speaking, all you are doing is going out to a third party data vendor and fetching additional data based on some common key. One of the most keys for data enrichment is searching based on email address or company&amp;rsquo;s domain address.</description>
    </item>
    
    <item>
      <title>How to create pdf documents in python using FPDF library</title>
      <link>http://jaympatel.com/2020/08/how-to-create-pdf-documents-in-python-using-fpdf-library/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>http://jaympatel.com/2020/08/how-to-create-pdf-documents-in-python-using-fpdf-library/</guid>
      <description>PDF files are ubiquitous in our daily life and it&amp;rsquo;s a good idea to spend few minutes to learn how to programmatically generate it in Python.
In this tutorial, we will take structured data in JSON and convert that into a pdf. Ideally, pdfs contain text paragraphs, hyperlinks and images, so we will work with a dataset containing all of that for better real world experience.
Firstly, let us fetch the data from a publicly available API containing scraped news articles from past 24 hours.</description>
    </item>
    
  </channel>
</rss>