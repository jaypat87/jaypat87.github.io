<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Processing on Jay M. Patel</title>
    <link>http://jaympatel.com/categories/natural-language-processing/</link>
    <description>Recent content in Natural Language Processing on Jay M. Patel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <copyright>Jay M. Patel</copyright>
    <lastBuildDate>Fri, 01 Feb 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://jaympatel.com/categories/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Introduction to natural language processing: rule based methods, name entity recognition (NER), and text classification</title>
      <link>http://jaympatel.com/2019/02/introduction-to-natural-language-processing-rule-based-methods-name-entity-recognition-ner-and-text-classification/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://jaympatel.com/2019/02/introduction-to-natural-language-processing-rule-based-methods-name-entity-recognition-ner-and-text-classification/</guid>
      <description>The ability of computers to understand human languages us referred to as Natural Language Processing (NLP). This is a vast field and frequently practitioners include machine translation and natural language generation (NLG) as part of core NLP. However, in this section we will only look at NLP techniques which aim to extract insights from unstructured text.
Regular expressions (Regex) and rule based methods Regular expressions (Regex) match patterns with sequences of characters and they are supported in wide variety of programming languages.</description>
    </item>
    
    <item>
      <title>Natural language processing (NLP): word embeddings, words2vec, GloVe based text vectorization in python</title>
      <link>http://jaympatel.com/2019/02/natural-language-processing-nlp-word-embeddings-words2vec-glove-based-text-vectorization-in-python/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://jaympatel.com/2019/02/natural-language-processing-nlp-word-embeddings-words2vec-glove-based-text-vectorization-in-python/</guid>
      <description>In the previous posts, we looked at count vectorization and term frequency-inverse document frequency (tf-idf) to convert a given text document into a vectors which can be used as features for text classification task such as classifying emails into spam or not spam.
Major disadvantages of such an approach is that the vectors generated are sparse (mostly made of zeros), and very high-dimensional (same dimensionality as the number of words in the vocabulary).</description>
    </item>
    
    <item>
      <title>Natural language processing (NLP): Text Vectorization and Bag of Words Approach</title>
      <link>http://jaympatel.com/2019/02/natural-language-processing-nlp-text-vectorization-and-bag-of-words-approach/</link>
      <pubDate>Mon, 04 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://jaympatel.com/2019/02/natural-language-processing-nlp-text-vectorization-and-bag-of-words-approach/</guid>
      <description>Let us consider a simple task of classifying a given email into spam or not spam. As you might have already guessed, this is an example of a simple binary classification problem with target or label values being 0 or 1, within the larger field of supervised machine learning (ML).
However, we still need to convert non-numerical text contained in emails into numerical features which can be fed into a ML algorithm of our choice.</description>
    </item>
    
    <item>
      <title>Natural language processing (NLP): Term Frequency - Inverse Document Frequency (Tf-idf) based vectorization in python</title>
      <link>http://jaympatel.com/2019/02/natural-language-processing-nlp-term-frequency-inverse-document-frequency-tf-idf-based-vectorization-in-python/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://jaympatel.com/2019/02/natural-language-processing-nlp-term-frequency-inverse-document-frequency-tf-idf-based-vectorization-in-python/</guid>
      <description>In the previous post, we looked at count vectorization to convert a given document into a sparse vectors which can be used as features for text classification task such as classifying emails into spam or not spam.
Count vectorization has one major drawback: it gives equal weightage to all the words (or tokens) present in the corpus, and this makes it a poor representation for the semantic analysis of the sentence.</description>
    </item>
    
  </channel>
</rss>